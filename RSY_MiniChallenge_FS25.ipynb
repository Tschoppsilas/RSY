{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems - Mini Challenge HS24\n",
    "\n",
    "In this minichallenge we will explore a MovieLens dataset and implement several recommender systems and evaluation methods. Subsequently we will optimize these methods and compare the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission deadline:** 24.11.2024 18:00. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines for Implementation and Submission\n",
    "- Code must be written in Python. The versions of all used packages must be given for reproducability.\n",
    "- You may respond in English or German.\n",
    "- We develop numerous algorithms ourselves. Unless explicitly stated otherwise, only the following libraries may be used in Python: numpy, matplotlib, seaborn, pandas. \n",
    "- Follow good coding practices and write modular, reusable code.\n",
    "- The submitted solution must contain all codes and the results. No code may be outsourced.\n",
    "- All pathes must be relative and just downloading your repo must be executable without modifications.\n",
    "- Only fully running code is graded. The notebook must run sequential from start to end.\n",
    "- If computation time is too long for productive prototyping and debugging work, it is recommended to reduce the dataset to a fraction of its original. However, final results should be calculated on the full dataset. \n",
    "- All plots must be fully labeled (title, axes, labels, colorbar, etc.) so that the plot can be easily understood.\n",
    "- Each plot should be accompanied by a brief discussion, which explains the plot and captures the key insights that become visible.\n",
    "- Only fully labeled plots with an accompanying discussion will be assessed.\n",
    "- The last commit in your fork of the repo before the submission deadline counts as the submission.\n",
    "- Points will be deducted if you write inconsise (Denial of service will be punished) or if I read the text that are not written for me but for the user of ChatGPT. \n",
    "- If you would like to submit and have the mini-challenge assessed, please send a short email to the subject expert (moritz.kirschmann@fhnw.ch) within 2 days after submission.\n",
    "- Please do not delete, duplicate, or move the existing cells. This leads to problems during the correction. However, you may add as many additional cells as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - A deep exploration of the dataset (17 points)\n",
    "We will work with a subset of the MovieLens dataset. This subset is located under ``data/ml-latest-small``. Read the ``README.txt``carefully. \n",
    "Open the files. \n",
    "\n",
    "a) Describe the available data.\n",
    "\n",
    "Die Daten von MovieLens, einem Filmempfehlungsdienst, beschreiben 5-Sterne-Bewertungen von Filmen. Er enthält 100’836 Bewertungen und 3’683 Tags-Anwendungen für 9’742 Filme. Diese Daten wurden von 610 Nutzern zwischen dem 29. März 1996 und dem 24. September 2018 erstellt. Der Datensatz wurde am 26. September 2018 erstellt.\n",
    "Die Nutzer wurden nach dem Zufallsprinzip für die Aufnahme ausgewählt. Alle ausgewählten Nutzer hatten mindestens 20 Filme bewertet. Es sind keine demografischen Informationen enthalten. Jeder Nutzer wird durch eine ID repräsentiert, und es werden keine weiteren Informationen bereitgestellt.\n",
    "Die Daten sind in den Dateien «links.csv», «movies.csv», «ratings.csv» und «tags.csv» enthalten. \n",
    "Die Daten:\n",
    "•\tratings.csv: Enthält alle Bewertungen. Jede Zeile repräsentiert eine Bewertung eines Films durch einen Benutzer und hat das Format: «userId», «movieId», «rating» und «timestamp». Die Bewertungen erfolgen auf einer 5-Sterne-Skala mit Abstufungen von einem halben Stern (0,5 Sterne - 5,0 Sterne).\n",
    "•\ttags.csv: Enthält alle tags (Etikett). Jede Zeile repräsentiert ein tag, das von einem Benutzer auf einen Film angewendet wurde, und hat das Format: «userId», «movieId», «tag» und «timestamp». Tags sind vom Benutzer erstellte Metadaten zu Filmen, wie beispielsweise der Hauptdarsteller oder ein Wort das beschreibt um was es im Gilm geht (bsp. Wedding wenn im Film geheiratet wird).\n",
    "•\tmovies.csv: Enthält Filminformationen. Jede Zeile repräsentiert einen Film und hat das Format: «movieId», «title» und «genres». Die Genres sind eine durch Pipes ( | ) getrennte Liste, welche genres vom Film alles abgedeckt werden.\n",
    "•\tlinks.csv: Enthält Bezeichner zur Verknüpfung mit anderen Quellen von Filmdaten. Jede Zeile repräsentiert einen Film und hat das Format: «movieId», «imdbId» (imbdId = Internet Movie Database ID) und «tmdbId» (tmdbid = The Movie Database ID). Diese IDs helfen dabei, einen Film eindeutig auf verschiedenen Plattformen zu identifizieren. Wenn man also eine bestimmte Filminformation abrufen möchte, kann man die jeweilige ID nutzen, um den Film auf einer der Plattformen zu finden.\n",
    "\n",
    "Programmiertechnisch wird der Datensatz im Abschnitt unter dem Markdown \"a) Describe the available data\" gezeigt.\n",
    "\n",
    "\n",
    "b) Find and fix bad data (e.g. duplicates, missing values, etc.).\n",
    "\n",
    "Generate lists of\n",
    "\n",
    "c) - Top 20 movies by average rating\n",
    "\n",
    "d) - Top 20 movies by number of views\n",
    "\n",
    "e) What is the range of the ratings? \n",
    "\n",
    "f) Which genre has be rated how many times?\n",
    "\n",
    "g) How sparse is the User Rating Matrix?\n",
    "\n",
    "Plot the following:\n",
    "\n",
    "h) How many users have rated how many movies\n",
    "\n",
    "i) Which rating is given how often on average\n",
    "\n",
    "j) Which rating is given how often on average per genre\n",
    "\n",
    "k) The rating distributions of 10 random movies\n",
    "\n",
    "l) The rating distributions of 3 movies that you have watched\n",
    "\n",
    "m) How many users give which average rating\n",
    "\n",
    "n) How often a movie was rated as a function of average rating\n",
    "\n",
    "o) A heatmap of the User Item Matrix\n",
    "\n",
    "p) A heatmap of the User Item Matrix for the 100 most rated movies for the 50 users with most ratings\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datensätze werden zum ersten Mal eingelesen"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:20:10.552245Z",
     "start_time": "2025-04-02T13:20:10.458679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import groupby\n",
    "\n",
    "#Datensätze einlesen\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "output_dir = Path(\"Data\\ml-latest-small\")\n",
    "csv_path_links = output_dir / \"links.csv\"\n",
    "csv_path_movies = output_dir / \"movies.csv\"\n",
    "csv_path_ratings = output_dir / \"ratings.csv\"\n",
    "csv_path_tags = output_dir / \"tags.csv\"\n",
    "links_raw = pd.read_csv(csv_path_links)\n",
    "movies_raw = pd.read_csv(csv_path_movies)\n",
    "ratings_raw = pd.read_csv(csv_path_ratings)\n",
    "tags_raw = pd.read_csv(csv_path_tags)"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## a) Describe the available data."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T12:40:03.793755Z",
     "start_time": "2025-04-02T12:40:03.777428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Aufgabe A\n",
    "def data_describe(df):\n",
    "    print(f\"{df} Datensatz\")\n",
    "    print(df.info())\n",
    "\n",
    "data_describe(links_raw)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId   imdbId    tmdbId\n",
      "0           1   114709     862.0\n",
      "1           2   113497    8844.0\n",
      "2           3   113228   15602.0\n",
      "3           4   114885   31357.0\n",
      "4           5   113041   11862.0\n",
      "...       ...      ...       ...\n",
      "9729   193581  5476944  432131.0\n",
      "9730   193583  5914996  445030.0\n",
      "9731   193585  6397426  479308.0\n",
      "9732   193587  8391976  483455.0\n",
      "9733   193609   101726   37891.0\n",
      "\n",
      "[9734 rows x 3 columns] Datensatz\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9734 entries, 0 to 9733\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   movieId  9734 non-null   int64  \n",
      " 1   imdbId   9734 non-null   int64  \n",
      " 2   tmdbId   9734 non-null   float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 228.3 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T12:40:09.456465Z",
     "start_time": "2025-04-02T12:40:09.433657Z"
    }
   },
   "cell_type": "code",
   "source": "data_describe(movies_raw)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId                                      title  \\\n",
      "0           1                           Toy Story (1995)   \n",
      "1           2                             Jumanji (1995)   \n",
      "2           3                    Grumpier Old Men (1995)   \n",
      "3           4                   Waiting to Exhale (1995)   \n",
      "4           5         Father of the Bride Part II (1995)   \n",
      "...       ...                                        ...   \n",
      "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
      "9738   193583               No Game No Life: Zero (2017)   \n",
      "9739   193585                               Flint (2017)   \n",
      "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
      "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
      "\n",
      "                                           genres  \n",
      "0     Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                      Adventure|Children|Fantasy  \n",
      "2                                  Comedy|Romance  \n",
      "3                            Comedy|Drama|Romance  \n",
      "4                                          Comedy  \n",
      "...                                           ...  \n",
      "9737              Action|Animation|Comedy|Fantasy  \n",
      "9738                     Animation|Comedy|Fantasy  \n",
      "9739                                        Drama  \n",
      "9740                             Action|Animation  \n",
      "9741                                       Comedy  \n",
      "\n",
      "[9742 rows x 3 columns] Datensatz\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9742 entries, 0 to 9741\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   movieId  9742 non-null   int64 \n",
      " 1   title    9742 non-null   object\n",
      " 2   genres   9742 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 228.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T12:40:11.178985Z",
     "start_time": "2025-04-02T12:40:11.157208Z"
    }
   },
   "cell_type": "code",
   "source": "data_describe(ratings_raw)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100836 rows x 4 columns] Datensatz\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T12:40:23.061125Z",
     "start_time": "2025-04-02T12:40:23.032752Z"
    }
   },
   "cell_type": "code",
   "source": "data_describe(tags_raw)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      userId  movieId               tag   timestamp\n",
      "0          2    60756             funny  1445714994\n",
      "1          2    60756   Highly quotable  1445714996\n",
      "2          2    60756      will ferrell  1445714992\n",
      "3          2    89774      Boxing story  1445715207\n",
      "4          2    89774               MMA  1445715200\n",
      "...      ...      ...               ...         ...\n",
      "3678     606     7382         for katie  1171234019\n",
      "3679     606     7936           austere  1173392334\n",
      "3680     610     3265            gun fu  1493843984\n",
      "3681     610     3265  heroic bloodshed  1493843978\n",
      "3682     610   168248  Heroic Bloodshed  1493844270\n",
      "\n",
      "[3683 rows x 4 columns] Datensatz\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3683 entries, 0 to 3682\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   userId     3683 non-null   int64 \n",
      " 1   movieId    3683 non-null   int64 \n",
      " 2   tag        3683 non-null   object\n",
      " 3   timestamp  3683 non-null   int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 115.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## b) Find and fix bad data (e.g. duplicates, missing values, etc.)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:15:12.700813Z",
     "start_time": "2025-04-02T14:15:11.686217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Aufgabe B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Pfade der Datensätze um sicherzugehen, dass ich die nehme, die noch unberührt sind\n",
    "files = {\n",
    "    \"links\": \"Data/ml-latest-small/links.csv\",\n",
    "    \"movies\": \"Data/ml-latest-small/movies.csv\",\n",
    "    \"ratings\": \"Data/ml-latest-small/ratings.csv\",\n",
    "    \"tags\": \"Data/ml-latest-small/tags.csv\",\n",
    "}\n",
    "\n",
    "# Funktion zur Bereinigung und Standardisierung von CSV-Daten\n",
    "def clean_dataframe(df):\n",
    "    # Entfernen von unnötigen Anführungszeichen, die den gesamten Wert umschliessen, da mir auffiel, dass es im Datensatz Movies dieses Problem gab\n",
    "    def clean_string(x):\n",
    "        if isinstance(x, str):\n",
    "            # Entfernen von führenden und nachfolgenden Anführungszeichen\n",
    "            x = x.strip(\"'\\\"\")\n",
    "        return x\n",
    "\n",
    "    df = df.map(clean_string)\n",
    "    \n",
    "    # mögliche \"<unset>\" durch NaN ersetzen\n",
    "    df = df.replace(\"<unset>\", np.nan)\n",
    "    \n",
    "    # Überprüfen und Anpassen des Datentyps\n",
    "    for column in df.columns:\n",
    "        # Versuchen, den Datentyp zu inferieren und zu casten, falls der Datentyp nicht konsistent ist\n",
    "        if df[column].dtype == 'object':\n",
    "            # Überprüfen, ob die Spalte nur numerische Werte enthält\n",
    "            try:\n",
    "                df[column] = pd.to_numeric(df[column], errors='raise')\n",
    "            except ValueError:\n",
    "                pass  # Wenn es nicht konvertierbar ist, behalten wir den ursprünglichen Datentyp bei\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Lade Daten in DataFrames und stelle sicher, dass die erste Zeile als Spaltennamen verwendet wird\n",
    "dfs = {}\n",
    "for name, path in files.items():\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    \n",
    "    # Setze die erste Zeile als Spaltennamen\n",
    "    df.columns = df.iloc[0]\n",
    "    \n",
    "    # Entferne die erste Zeile, da sie jetzt die Spaltenüberschriften sind\n",
    "    df = df.drop(0, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    dfs[name] = df\n",
    "\n",
    "# Anwenden der Reinigung auf alle DataFrames\n",
    "dfs_cleaned = {name: clean_dataframe(df) for name, df in dfs.items()}\n",
    "\n",
    "# Zielordner definieren um die neuen DF von den alten getrennt zu speichern\n",
    "output_dir = \"Data/filtered_data\"\n",
    "\n",
    "# Sicherstellen, dass der Zielordner existiert\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for name, df in dfs_cleaned.items():\n",
    "    # Berechnen der Anzahl der Duplikate und NA aus den gefilterten Daten um zu zeigen, dass sie gut gefiltert wurden\n",
    "    duplicates_count = df.duplicated().sum()\n",
    "    na_count = df.isna().sum()\n",
    "    # Ausgabe der Anzahl der Duplikate\n",
    "    print(f\"Anzahl der Duplikate im {name}-Datensatz: {duplicates_count}\\n\")\n",
    "    # Ausgabe der Anzahl der NA\n",
    "    print(f\"Anzahl der NA im {name}-Datensatz: {na_count}\\n\")\n",
    "    \n",
    "    # Speichern der bereinigten DataFrames als CSV-Dateien\n",
    "    df.to_csv(os.path.join(output_dir, f\"{name}_cleaned.csv\"), index=False)\n",
    "\n",
    "print(\"Bereinigte Daten wurden gespeichert.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Duplikate im links-Datensatz: 0\n",
      "\n",
      "Anzahl der NA im links-Datensatz: 0\n",
      "movieId    0\n",
      "imdbId     0\n",
      "tmdbId     0\n",
      "dtype: int64\n",
      "\n",
      "Anzahl der Duplikate im movies-Datensatz: 0\n",
      "\n",
      "Anzahl der NA im movies-Datensatz: 0\n",
      "movieId    0\n",
      "title      0\n",
      "genres     0\n",
      "dtype: int64\n",
      "\n",
      "Anzahl der Duplikate im ratings-Datensatz: 0\n",
      "\n",
      "Anzahl der NA im ratings-Datensatz: 0\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "Anzahl der Duplikate im tags-Datensatz: 0\n",
      "\n",
      "Anzahl der NA im tags-Datensatz: 0\n",
      "userId       0\n",
      "movieId      0\n",
      "tag          0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "Bereinigte Daten wurden gespeichert.\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:51:32.529734Z",
     "start_time": "2025-04-02T14:51:32.439168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Gefilterte Datensätze wieder den Variablen zuweisen:\n",
    "output_dir = Path(\"Data/filtered_data\")\n",
    "csv_path_links = output_dir / \"links_cleaned.csv\"\n",
    "csv_path_movies = output_dir / \"movies_cleaned.csv\"\n",
    "csv_path_ratings = output_dir / \"ratings_cleaned.csv\"\n",
    "csv_path_tags = output_dir / \"tags_cleaned.csv\"\n",
    "links = pd.read_csv(csv_path_links)\n",
    "movies = pd.read_csv(csv_path_movies)\n",
    "ratings = pd.read_csv(csv_path_ratings)\n",
    "tags = pd.read_csv(csv_path_tags)"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## c) Generate a lis of Top 20 movies by average rating"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T15:09:18.934194Z",
     "start_time": "2025-04-02T15:09:18.911450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Damit die Liste im Output besser aussieht brauche ich pprint\n",
    "import pprint\n",
    "#gruppiert die Filme und berechnet den Bewertungsdurchschnitt. Zusätzlich, nehme ich nur die höchsten 20 und wandle es zu einem DF um, damit ich es mergen kann\n",
    "top_20_movies = ratings.groupby(\"movieId\")[\"rating\"].mean().sort_values(ascending=False).head(20).reset_index()\n",
    "\n",
    "liste_top_20 = list(top_20_movies.merge(movies[[\"movieId\", \"title\"]], on=\"movieId\", how=\"left\")[\"title\"])\n",
    "pprint.pprint(liste_top_20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Won't You Be My Neighbor? (2018)\",\n",
      " 'Jane Eyre (1944)',\n",
      " 'Rain (2001)',\n",
      " 'Goodbye Charlie (1964)',\n",
      " 'Sorority House Massacre (1986)',\n",
      " 'Slumber Party Massacre III (1990)',\n",
      " 'Slumber Party Massacre II (1987)',\n",
      " 'True Stories (1986)',\n",
      " 'Moonlight',\n",
      " 'Tom Segura: Mostly Stories (2016)',\n",
      " 'All Yours (2016)',\n",
      " 'Valet, The (La doublure) (2006)',\n",
      " 'Odd Life of Timothy Green, The (2012)',\n",
      " 'Lady Jane (1986)',\n",
      " 'Open Hearts (Elsker dig for evigt) (2002)',\n",
      " 'SORI: Voice from the Heart (2016)',\n",
      " 'Tenchi Muyô! In Love (1996)',\n",
      " 'A Flintstones Christmas Carol (1994)',\n",
      " 'Delirium (2014)',\n",
      " 'Snowflake, the White Gorilla (2011)']\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## d) Generate a list of Top 20 movies by number of views\n",
    "Ich gehe hier davon aus, dass ein view einer Bewertung entspricht, da ein Film für eine Bewertung zumindest einmal gesehen werden muss."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T15:23:41.455906Z",
     "start_time": "2025-04-02T15:23:41.433326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gruppiert die Filme und summiert die ANzahl Bewertungen. Zusätzlich, nehme ich nur die höchsten 20 und wandle es zu einem DF um, damit ich es mergen kann\n",
    "top_20_movies_by_view = ratings.groupby(\"movieId\")[\"rating\"].count().sort_values(ascending=False).head(20).reset_index()\n",
    "\n",
    "liste_top_20_by_view = list(top_20_movies_by_view.merge(movies[[\"movieId\", \"title\"]], on=\"movieId\", how=\"left\")[\"title\"])\n",
    "pprint.pprint(liste_top_20_by_view)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Forrest Gump (1994)',\n",
      " 'Shawshank Redemption, The (1994)',\n",
      " 'Pulp Fiction (1994)',\n",
      " 'Silence of the Lambs, The (1991)',\n",
      " 'Matrix, The (1999)',\n",
      " 'Star Wars: Episode IV - A New Hope (1977)',\n",
      " 'Jurassic Park (1993)',\n",
      " 'Braveheart (1995)',\n",
      " 'Terminator 2: Judgment Day (1991)',\n",
      " \"Schindler's List (1993)\",\n",
      " 'Fight Club (1999)',\n",
      " 'Toy Story (1995)',\n",
      " 'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
      " 'American Beauty (1999)',\n",
      " 'Usual Suspects, The (1995)',\n",
      " 'Seven (a.k.a. Se7en) (1995)',\n",
      " 'Independence Day (a.k.a. ID4) (1996)',\n",
      " 'Apollo 13 (1995)',\n",
      " 'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) '\n",
      " '(1981)',\n",
      " 'Lord of the Rings: The Fellowship of the Ring, The (2001)']\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## e) What is the range of the ratings?"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T15:55:36.073193Z",
     "start_time": "2025-04-02T15:55:36.061131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_of_ratings = ratings[\"rating\"].drop_duplicates().sort_values(ascending=False).tolist()\n",
    "print(f\"Die Spannweite der Bewertungen für die Filme liegt zwischen: {list_of_ratings[-1]} bis {list_of_ratings[0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Spannweite der Bewertungen für die Filme liegt zwischen: 0.5 bis 5.0\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## f) Which genre has be rated how many times?"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:12:15.860182Z",
     "start_time": "2025-04-02T16:12:15.292412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "#genres aus movies in ratings datensatz einbauen\n",
    "ratings_mit_gernes = ratings.merge(movies[[\"movieId\", \"genres\"]], on=\"movieId\", how=\"left\")\n",
    "\n",
    "# Die Genre sind teilweise mit | getrennt, was sich, wenn es nicht aufgesplittet wird, nicht sauber zählen lässt, deshalb muss es hier aufgesplittet werden\n",
    "ratings_mit_gernes_exploded = ratings_mit_gernes.assign(genre=df[\"genres\"].str.split(\"|\")).explode(\"genre\")\n",
    "\n",
    "#Anzahl der Bewertungen pro Genre zählen\n",
    "genre_counts = ratings_mit_gernes_exploded[\"genre\"].value_counts().reset_index(name=\"count\")\n",
    "\n",
    "pprint.pprint(genre_counts)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 genre  count\n",
      "0                Drama  41928\n",
      "1               Comedy  39053\n",
      "2               Action  30635\n",
      "3             Thriller  26452\n",
      "4            Adventure  24161\n",
      "5              Romance  18124\n",
      "6               Sci-Fi  17243\n",
      "7                Crime  16681\n",
      "8              Fantasy  11834\n",
      "9             Children   9208\n",
      "10             Mystery   7674\n",
      "11              Horror   7291\n",
      "12           Animation   6988\n",
      "13                 War   4859\n",
      "14                IMAX   4145\n",
      "15             Musical   4138\n",
      "16             Western   1930\n",
      "17         Documentary   1219\n",
      "18           Film-Noir    870\n",
      "19  (no genres listed)     47\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## g) How sparse is the User Rating Matrix?"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:22:53.285941Z",
     "start_time": "2025-04-02T16:22:53.266679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Anzahl der eindeutigen Benutzer und Filme\n",
    "num_users = ratings[\"userId\"].nunique()\n",
    "num_movies = ratings[\"movieId\"].nunique()\n",
    "\n",
    "# Anzahl der vorhandenen Bewertungen\n",
    "num_ratings = len(ratings)\n",
    "\n",
    "# Gesamtanzahl der möglichen Bewertungen\n",
    "total_mgl_ratings = num_users * num_movies\n",
    "\n",
    "\n",
    "# Berechnung der spärlichkeit\n",
    "spärlichkeit = 1 - (num_ratings / total_mgl_ratings)\n",
    "\n",
    "print(f\"Es fehlen rund {round(spärlichkeit,4) *100}% aller möglichen Bewertungen\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es fehlen rund 98.3% aller möglichen Bewertungen\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Building a baseline RS (7 points)\n",
    "In this exercise we will build a baseline RS and functions to calculate fundamental performance metrics. \n",
    "\n",
    "Build the following baseline RS to predict Top-N (default N=20):\n",
    "1. In reference to the book *Collaborative Filtering Recommender Systems by Michael D. Ekstrand, John T. Riedl and Joseph A. Konstan* (p. 91ff) implement the baseline predictor $$ b_{u,i}= \\mu +b_u +b_i $$ with the regularized user and item average offsets: $$ b_u = \\frac{1}{|I_u| + \\beta_u} \\sum_{i \\in I_u} (r_{u,i} - \\mu) $$ and $$ b_i = \\frac{1}{|U_i| + \\beta_i} \\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu) . $$ Build a recommender system upon this baseline predictor. Set the default damping factors $\\beta_u$ and $\\beta_i$ both to 20.\n",
    "2. Build a RS that recommends based on *random* recommendations.  \n",
    "\n",
    "Output the recommendations for three example users (Ids 1, 3 and 7) and the default parameters. Give the titles of the recommended movies and their predicted scores not just their Ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Evaluation methods (12 points)\n",
    "Split the data into train/validation set and a separate test set. This test set shall contain the first 20% of the users and shall not be used at all before exercise 10. With the remaining 80% do the following: \n",
    "Implement a function to partition your dataset for an offline evaluation based on holding out of random users with 5x cross validation with a 80/20 train/validation split. Within the validation set implement a masking with *all but n* approach. \n",
    "See page 2942 of https://jmlr.csail.mit.edu/papers/volume10/gunawardana09a/gunawardana09a.pdf for details on this approach. \n",
    "\n",
    "Choose the number of masked items n reasonably and explain your considerations.\n",
    "\n",
    "Implement functions to calculate the following metrics:\n",
    "- *Mean Absolute Error (MAE)* \n",
    "- *Root Mean Square Error (RMSE)*\n",
    "- *Precision@N* with default $N=15$ and relevance threshold 4.0 stars.\n",
    "- *Recall@N* with default $N=15$ and relevance threshold 4.0 stars.\n",
    "\n",
    "Explain each of these. How does the relevance threshold influence the metrics? How would you choose this parameter?\n",
    "\n",
    "Note: For the last two metrics use the definitions from https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54 with one exception: In case of the denominator being zero, set the metric to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Optimize hyperparameters of baseline RS (6 points)\n",
    "Optimize the hyperparameters $\\beta_u$ and $\\beta_i$ for the baseline RS from exercise 2 based on the RMSE metric. To save computation time find a reasonable maximum value for the betas. Explain your approach and your solution.\n",
    "Plot the MAE, RMSE, Precision@N, Recall@N as functions of the betas.\n",
    "\n",
    "Which metric would you use for hyperparameter tuning? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Collaborative filtering; item-based and user-based (12 points)\n",
    "In this exersise we will build several different collaborative-filtering RS based on nearest neighbour technique, both in terms of item and user. \n",
    "\n",
    "Implement:\n",
    "1. a RS based on the $K$ most similar items (K nearest neighbours). Similarity shall be calculated based on *cosine similarity*. \n",
    "2. a RS based on the $K$ most similar items (K nearest neighbours). Similarity shall be calculated based on *Pearson Correlation Coefficienct*. \n",
    "3. a RS based on the $K$ most similar users (K nearest neighbours). Similarity shall be calculated based on *cosine similarity*. \n",
    "4. a RS based on the $K$ most similar users (K nearest neighbours). Similarity shall be calculated based on *Pearson Correlation Coefficienct*. \n",
    "\n",
    "Each should have a default $K$ of 30.\n",
    "\n",
    "Explain how you handle NaN values in the user rating matrix when computing similarities? What other preparations are useful such as normalization and mean centering?\n",
    "\n",
    "Describe the two similarity metrics.\n",
    "\n",
    "Show the top 20 recommended items for user ids 3, 5 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 6 - Optimize hyperparameter $K$ (6 points)\n",
    "Optimize the hyperparameter $K$ for all RS from the prior exercise optimizing for minimal RMSE. \n",
    "For each RS plot RMSE, Precision@N and Recall@N as a function of $K$. \n",
    "\n",
    "Compare the results of these four RS on the 3 example users. Do the results match your expectation? Describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - Model-based RS: SVD (10 points)\n",
    "In this exercise we will use the unsupervised method *singular value decomposition (SVD)* from the python package *surprise* (https://surpriselib.com, documentation https://surprise.readthedocs.io/en/stable/matrix_factorization.html). SVD can compress much of the information of a matrix in few components.  \n",
    "\n",
    "a)Run the SVD RS and show the results on the three example users from exercise 2. Explain how this algorithm works.\n",
    "\n",
    "Note: A very good general introduction to SVD is this youtube video series starting with https://www.youtube.com/watch?v=gXbThCXjZFM&t=337s . See *Collaborative filtering recommender systems* by Ekstrand et al. *Mining of massive datasets* by Leskovec, Kapitel 11 (2020) and, *Recommender systems: The textbook*, by Aggarwal, chapter 3\n",
    "\n",
    "b) We explore now what latent factors SVD has learned. Generate three sorted lists: Sort the items by their biggest, second biggest and third biggest singlular value component. For each list print the top and bottom 20 items. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 - Optimize hyperparameter $k$ or `n_factors` (4 points)\n",
    "Optimize the hyperparameter, representing the number of greatest SVD components used for the truncated reconstruction of the user item matrix, to minimize RMSE.\n",
    "Plot RMSE, Precision@N and Recall@N as a function of this hyperparameter. Finally output all performance metrics from exercise 3 for the optimal $k$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 - Everything goes (30 points)\n",
    "In this exercise you can explore different methods of RS. You are not limited what methods you apply. You can try to improve the methods from the earlier exercises by modifiying them or generating ensemble or hybrid RS. Also you could train deep neural networks, use NLP methods, use the available links to imdb available in the dataset to further enrich the dataset or find an obscure method by someone else on Github. \n",
    "Document what your inspirations and sources are and describe the method conceptually. \n",
    "\n",
    "**Build and optimize in total *three* different methods. The last one has the additional requirement that it should increase the diversity of the recommendations in order to minimize filter bubbles.**\n",
    "\n",
    "**Important: If you use the work of someone else you must be able to explain the method conceptually during the defense MSP.** \n",
    "\n",
    "Output the performance metrics of exercise 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 - Compare all RS that you build in this challenge (8 points)\n",
    "a) Compile a table with the performance metrics of exercise 3 for all RS from this MC (Make sure to include the baseline RS and random RS) on the test set defined in exercise 3. Also generate comparative plots. Discuss.\n",
    "\n",
    "b) Why is it important to keep a test set seperate till the end of a benchmark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the Guidelines for Implementation and Submission one more time.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSY_FS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems - Mini Challenge HS24\n",
    "\n",
    "In this minichallenge we will explore a MovieLens dataset and implement several recommender systems and evaluation methods. Subsequently we will optimize these methods and compare the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission deadline:** 24.11.2024 18:00. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines for Implementation and Submission\n",
    "- Code must be written in Python. The versions of all used packages must be given for reproducability.\n",
    "- You may respond in English or German.\n",
    "- We develop numerous algorithms ourselves. Unless explicitly stated otherwise, only the following libraries may be used in Python: numpy, matplotlib, seaborn, pandas. \n",
    "- Follow good coding practices and write modular, reusable code.\n",
    "- The submitted solution must contain all codes and the results. No code may be outsourced.\n",
    "- All pathes must be relative and just downloading your repo must be executable without modifications.\n",
    "- Only fully running code is graded. The notebook must run sequential from start to end.\n",
    "- If computation time is too long for productive prototyping and debugging work, it is recommended to reduce the dataset to a fraction of its original. However, final results should be calculated on the full dataset. \n",
    "- All plots must be fully labeled (title, axes, labels, colorbar, etc.) so that the plot can be easily understood.\n",
    "- Each plot should be accompanied by a brief discussion, which explains the plot and captures the key insights that become visible.\n",
    "- Only fully labeled plots with an accompanying discussion will be assessed.\n",
    "- The last commit in your fork of the repo before the submission deadline counts as the submission.\n",
    "- Points will be deducted if you write inconsise (Denial of service will be punished) or if I read the text that are not written for me but for the user of ChatGPT. \n",
    "- If you would like to submit and have the mini-challenge assessed, please send a short email to the subject expert (moritz.kirschmann@fhnw.ch) within 2 days after submission.\n",
    "- Please do not delete, duplicate, or move the existing cells. This leads to problems during the correction. However, you may add as many additional cells as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - A deep exploration of the dataset (17 points)\n",
    "We will work with a subset of the MovieLens dataset. This subset is located under ``data/ml-latest-small``. Read the ``README.txt``carefully. \n",
    "Open the files. \n",
    "\n",
    "a) Describe the available data.\n",
    "\n",
    "b) Find and fix bad data (e.g. duplicates, missing values, etc.).\n",
    "\n",
    "Generate lists of\n",
    "\n",
    "c) - Top 20 movies by average rating\n",
    "\n",
    "d) - Top 20 movies by number of views\n",
    "\n",
    "e) What is the range of the ratings? \n",
    "\n",
    "f) Which genre has be rated how many times?\n",
    "\n",
    "g) How sparse is the User Rating Matrix?\n",
    "\n",
    "Plot the following:\n",
    "\n",
    "h) How many users have rated how many movies\n",
    "\n",
    "i) Which rating is given how often on average\n",
    "\n",
    "j) Which rating is given how often on average per genre\n",
    "\n",
    "k) The rating distributions of 10 random movies\n",
    "\n",
    "l) The rating distributions of 3 movies that you have watched\n",
    "\n",
    "m) How many users give which average rating\n",
    "\n",
    "n) How often a movie was rated as a function of average rating\n",
    "\n",
    "o) A heatmap of the User Item Matrix\n",
    "\n",
    "p) A heatmap of the User Item Matrix for the 100 most rated movies for the 50 users with most ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Building a baseline RS (7 points)\n",
    "In this exercise we will build a baseline RS and functions to calculate fundamental performance metrics. \n",
    "\n",
    "Build the following baseline RS to predict Top-N (default N=20):\n",
    "1. In reference to the book *Collaborative Filtering Recommender Systems by Michael D. Ekstrand, John T. Riedl and Joseph A. Konstan* (p. 91ff) implement the baseline predictor $$ b_{u,i}= \\mu +b_u +b_i $$ with the regularized user and item average offsets: $$ b_u = \\frac{1}{|I_u| + \\beta_u} \\sum_{i \\in I_u} (r_{u,i} - \\mu) $$ and $$ b_i = \\frac{1}{|U_i| + \\beta_i} \\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu) . $$ Build a recommender system upon this baseline predictor. Set the default damping factors $\\beta_u$ and $\\beta_i$ both to 20.\n",
    "2. Build a RS that recommends based on *random* recommendations.  \n",
    "\n",
    "Output the recommendations for three example users (Ids 1, 3 and 7) and the default parameters. Give the titles of the recommended movies and their predicted scores not just their Ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Evaluation methods (12 points)\n",
    "Split the data into train/validation set and a separate test set. This test set shall contain the first 20% of the users and shall not be used at all before exercise 10. With the remaining 80% do the following: \n",
    "Implement a function to partition your dataset for an offline evaluation based on holding out of random users with 5x cross validation with a 80/20 train/validation split. Within the validation set implement a masking with *all but n* approach. \n",
    "See page 2942 of https://jmlr.csail.mit.edu/papers/volume10/gunawardana09a/gunawardana09a.pdf for details on this approach. \n",
    "\n",
    "Choose the number of masked items n reasonably and explain your considerations.\n",
    "\n",
    "Implement functions to calculate the following metrics:\n",
    "- *Mean Absolute Error (MAE)* \n",
    "- *Root Mean Square Error (RMSE)*\n",
    "- *Precision@N* with default $N=15$ and relevance threshold 4.0 stars.\n",
    "- *Recall@N* with default $N=15$ and relevance threshold 4.0 stars.\n",
    "\n",
    "Explain each of these. How does the relevance threshold influence the metrics? How would you choose this parameter?\n",
    "\n",
    "Note: For the last two metrics use the definitions from https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54 with one exception: In case of the denominator being zero, set the metric to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Optimize hyperparameters of baseline RS (6 points)\n",
    "Optimize the hyperparameters $\\beta_u$ and $\\beta_i$ for the baseline RS from exercise 2 based on the RMSE metric. To save computation time find a reasonable maximum value for the betas. Explain your approach and your solution.\n",
    "Plot the MAE, RMSE, Precision@N, Recall@N as functions of the betas.\n",
    "\n",
    "Which metric would you use for hyperparameter tuning? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Collaborative filtering; item-based and user-based (12 points)\n",
    "In this exersise we will build several different collaborative-filtering RS based on nearest neighbour technique, both in terms of item and user. \n",
    "\n",
    "Implement:\n",
    "1. a RS based on the $K$ most similar items (K nearest neighbours). Similarity shall be calculated based on *cosine similarity*. \n",
    "2. a RS based on the $K$ most similar items (K nearest neighbours). Similarity shall be calculated based on *Pearson Correlation Coefficienct*. \n",
    "3. a RS based on the $K$ most similar users (K nearest neighbours). Similarity shall be calculated based on *cosine similarity*. \n",
    "4. a RS based on the $K$ most similar users (K nearest neighbours). Similarity shall be calculated based on *Pearson Correlation Coefficienct*. \n",
    "\n",
    "Each should have a default $K$ of 30.\n",
    "\n",
    "Explain how you handle NaN values in the user rating matrix when computing similarities? What other preparations are useful such as normalization and mean centering?\n",
    "\n",
    "Describe the two similarity metrics.\n",
    "\n",
    "Show the top 20 recommended items for user ids 3, 5 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 6 - Optimize hyperparameter $K$ (6 points)\n",
    "Optimize the hyperparameter $K$ for all RS from the prior exercise optimizing for minimal RMSE. \n",
    "For each RS plot RMSE, Precision@N and Recall@N as a function of $K$. \n",
    "\n",
    "Compare the results of these four RS on the 3 example users. Do the results match your expectation? Describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - Model-based RS: SVD (10 points)\n",
    "In this exercise we will use the unsupervised method *singular value decomposition (SVD)* from the python package *surprise* (https://surpriselib.com, documentation https://surprise.readthedocs.io/en/stable/matrix_factorization.html). SVD can compress much of the information of a matrix in few components.  \n",
    "\n",
    "a)Run the SVD RS and show the results on the three example users from exercise 2. Explain how this algorithm works.\n",
    "\n",
    "Note: A very good general introduction to SVD is this youtube video series starting with https://www.youtube.com/watch?v=gXbThCXjZFM&t=337s . See *Collaborative filtering recommender systems* by Ekstrand et al. *Mining of massive datasets* by Leskovec, Kapitel 11 (2020) and, *Recommender systems: The textbook*, by Aggarwal, chapter 3\n",
    "\n",
    "b) We explore now what latent factors SVD has learned. Generate three sorted lists: Sort the items by their biggest, second biggest and third biggest singlular value component. For each list print the top and bottom 20 items. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 - Optimize hyperparameter $k$ or `n_factors` (4 points)\n",
    "Optimize the hyperparameter, representing the number of greatest SVD components used for the truncated reconstruction of the user item matrix, to minimize RMSE.\n",
    "Plot RMSE, Precision@N and Recall@N as a function of this hyperparameter. Finally output all performance metrics from exercise 3 for the optimal $k$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 - Everything goes (30 points)\n",
    "In this exercise you can explore different methods of RS. You are not limited what methods you apply. You can try to improve the methods from the earlier exercises by modifiying them or generating ensemble or hybrid RS. Also you could train deep neural networks, use NLP methods, use the available links to imdb available in the dataset to further enrich the dataset or find an obscure method by someone else on Github. \n",
    "Document what your inspirations and sources are and describe the method conceptually. \n",
    "\n",
    "**Build and optimize in total *three* different methods. The last one has the additional requirement that it should increase the diversity of the recommendations in order to minimize filter bubbles.**\n",
    "\n",
    "**Important: If you use the work of someone else you must be able to explain the method conceptually during the defense MSP.** \n",
    "\n",
    "Output the performance metrics of exercise 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 - Compare all RS that you build in this challenge (8 points)\n",
    "a) Compile a table with the performance metrics of exercise 3 for all RS from this MC (Make sure to include the baseline RS and random RS) on the test set defined in exercise 3. Also generate comparative plots. Discuss.\n",
    "\n",
    "b) Why is it important to keep a test set seperate till the end of a benchmark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the Guidelines for Implementation and Submission one more time.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSY_FS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
